ML workloads - analysis

1. Horus: An Interference-aware Resource Manager for Deep Learning Systems: measure inteference + resource utlization by running together multiple different models (e.g. MObileNet, GoogLeNet, VGGNet, ResNet, LSTM, Transformer, etc.)

2. Effective Elastic Scaling of Deep Learning Workloads: Adapts cluster size and batch size. Contains a Table with some jobs characterized as compute or communication bound, and also provides benchmarks for each category - mixing also workloads from different categories (e.g. alexnet-communication bound, resnet-compute bound)

3. Memory system characterization of deep learning workloads: Memory footprint depends on the number of layers, number of weights/layer, batch size etc. The paper provides details for memory footprint and usage of various DNN (ResNet and Inception spend more time for memory operations than the others - vgg has larger mem footprint) 

4. Fathom: Reference Workloads for Modern Deep Learning Methods: provides breakdown of execution time operation for multiple ML algorithms (RNN, CNN, Autoencoder etc.). Some of them spend most time in data movement (and processing). These are: seq2seg, memnet, autoenc, deep speech

5. FusionStitching: Boosting Memory Intensive Computations for Deep Learning Workloads: Breakdown of execution time for some ML algorithms. Memory intensive operations seem to be part of BERT, TRANSFORMER (Google), DIEN (reccomendation)

6. Performance analysis of deep learning workloads using roofine trajectories: AlexNet, ResNet, Inception more memory bound. Better performance with Horovod compared to the paremeter server 
